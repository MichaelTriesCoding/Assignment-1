{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichaelTriesCoding/Assignments/blob/main/Assignment_6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7ASnl49xUAX"
      },
      "source": [
        "Problem 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNRxyzwyQlpe",
        "outputId": "a46c97e4-8698-4cb0-874f-abf9180624b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Training Loss: 0.8064823150634766, Validation Loss: 1.2806141376495361\n",
            "Epoch 500, Training Loss: 0.19035528600215912, Validation Loss: 0.46986350417137146\n",
            "Epoch 1000, Training Loss: 0.1793081909418106, Validation Loss: 0.4803594946861267\n",
            "Epoch 1500, Training Loss: 0.48446422815322876, Validation Loss: 0.4787915349006653\n",
            "Epoch 2000, Training Loss: 0.6190503239631653, Validation Loss: 0.4739662706851959\n",
            "Epoch 2500, Training Loss: 0.26236796379089355, Validation Loss: 0.4702877700328827\n",
            "Epoch 3000, Training Loss: 0.22271305322647095, Validation Loss: 0.4636828899383545\n",
            "Epoch 3500, Training Loss: 0.29255741834640503, Validation Loss: 0.4604591131210327\n",
            "Epoch 4000, Training Loss: 0.07461673021316528, Validation Loss: 0.4570489525794983\n",
            "Epoch 4500, Training Loss: 0.1600130945444107, Validation Loss: 0.4586847424507141\n",
            "Epoch 5000, Training Loss: 0.12648871541023254, Validation Loss: 0.4563995599746704\n",
            "Epoch 0, Training Loss: 0.5684584379196167, Validation Loss: 0.9683974981307983\n",
            "Epoch 500, Training Loss: 0.2706748843193054, Validation Loss: 0.47921204566955566\n",
            "Epoch 1000, Training Loss: 0.4377284049987793, Validation Loss: 0.49636539816856384\n",
            "Epoch 1500, Training Loss: 0.1621384620666504, Validation Loss: 0.4941295087337494\n",
            "Epoch 2000, Training Loss: 0.24783387780189514, Validation Loss: 0.4829654097557068\n",
            "Epoch 2500, Training Loss: 0.32223162055015564, Validation Loss: 0.4766237735748291\n",
            "Epoch 3000, Training Loss: 0.17531529068946838, Validation Loss: 0.48020678758621216\n",
            "Epoch 3500, Training Loss: 0.134708434343338, Validation Loss: 0.49832552671432495\n",
            "Epoch 4000, Training Loss: 0.049454979598522186, Validation Loss: 0.5219106078147888\n",
            "Epoch 4500, Training Loss: 0.09753884375095367, Validation Loss: 0.5354928374290466\n",
            "Epoch 5000, Training Loss: 0.040092770010232925, Validation Loss: 0.5483816266059875\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('Housing.csv')\n",
        "\n",
        "# Map string variables to binary values\n",
        "variable_list = ['mainroad', 'guestroom', 'basement', 'hotwaterheating', 'airconditioning', 'prefarea']\n",
        "\n",
        "def binary_mapping(x):\n",
        "    return x.map({'no': 0, 'yes': 1})\n",
        "\n",
        "data[variable_list] = data[variable_list].apply(binary_mapping)\n",
        "data = data.drop('furnishingstatus', axis=1)\n",
        "\n",
        "# Separate features and target variable\n",
        "y = data['price'].values\n",
        "x = data.drop('price', axis=1).values\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.8, random_state=42)\n",
        "\n",
        "# Standardize the input features\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_val = scaler.transform(x_val)\n",
        "\n",
        "# Standardize the output features\n",
        "scaler_y = StandardScaler()\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "y_val_scaled = scaler_y.fit_transform(y_val.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
        "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader for training and validation sets\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Define the model\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(x_train.shape[1], 32),\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(32, 1)\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5000\n",
        "for epoch in range(epochs + 1):\n",
        "    model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(x_val_tensor)\n",
        "        val_loss = criterion(val_outputs.squeeze(), y_val_tensor)\n",
        "        if epoch % 500 == 0:\n",
        "            print(f'Epoch: {epoch}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n",
        "\n",
        "# Pt.2: More complex model with additional hidden layers\n",
        "data = pd.read_csv('Housing.csv')\n",
        "\n",
        "# Map string variables to binary values\n",
        "data[variable_list] = data[variable_list].apply(binary_mapping)\n",
        "data = data.drop('furnishingstatus', axis=1)\n",
        "\n",
        "# Separate features and target variable\n",
        "y = data['price'].values\n",
        "x = data.drop('price', axis=1).values\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.8, random_state=42)\n",
        "\n",
        "# Standardize the input features\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_val = scaler.transform(x_val)\n",
        "\n",
        "# Standardize the output features\n",
        "y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1, 1)).flatten()\n",
        "y_val_scaled = scaler_y.fit_transform(y_val.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32)\n",
        "x_val_tensor = torch.tensor(x_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32)\n",
        "\n",
        "# Create DataLoader for training and validation sets\n",
        "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# Define the more complex model with additional hidden layers\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(x_train.shape[1], 32),  # First hidden layer with 32 neurons\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(32, 64),  # Second hidden layer with 64 neurons\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(64, 16),  # Third hidden layer with 16 neurons\n",
        "    nn.Tanh(),\n",
        "    nn.Linear(16, 1)  # Output layer with 1 neuron\n",
        ")\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs + 1):\n",
        "    model.train()\n",
        "    for inputs, targets in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs.squeeze(), targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_outputs = model(x_val_tensor)\n",
        "        val_loss = criterion(val_outputs.squeeze(), y_val_tensor)\n",
        "        if epoch % 500 == 0:\n",
        "            print(f'Epoch: {epoch}, Train Loss: {loss.item()}, Validation Loss: {val_loss.item()}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2mh9ox78aLl"
      },
      "source": [
        "Problem 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "THhlwfxlO2VG",
        "outputId": "c58d025b-a284-4b14-9284-5ac9d0ee015b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 34215232.75it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/300, Loss: 2.0003, Training Time: 15.20 seconds\n",
            "Epoch 2/300, Loss: 1.8658, Training Time: 12.73 seconds\n",
            "Epoch 3/300, Loss: 1.8205, Training Time: 12.72 seconds\n",
            "Epoch 4/300, Loss: 1.7933, Training Time: 12.62 seconds\n",
            "Epoch 5/300, Loss: 1.7719, Training Time: 12.99 seconds\n",
            "Epoch 6/300, Loss: 1.7561, Training Time: 12.59 seconds\n",
            "Epoch 7/300, Loss: 1.7414, Training Time: 12.71 seconds\n",
            "Epoch 8/300, Loss: 1.7297, Training Time: 12.62 seconds\n",
            "Epoch 9/300, Loss: 1.7183, Training Time: 12.38 seconds\n",
            "Epoch 10/300, Loss: 1.7074, Training Time: 12.51 seconds\n",
            "Epoch 11/300, Loss: 1.6980, Training Time: 12.78 seconds\n",
            "Epoch 12/300, Loss: 1.6890, Training Time: 12.44 seconds\n",
            "Epoch 13/300, Loss: 1.6806, Training Time: 12.40 seconds\n",
            "Epoch 14/300, Loss: 1.6724, Training Time: 12.68 seconds\n",
            "Epoch 15/300, Loss: 1.6642, Training Time: 12.36 seconds\n",
            "Epoch 16/300, Loss: 1.6570, Training Time: 12.52 seconds\n",
            "Epoch 17/300, Loss: 1.6496, Training Time: 12.79 seconds\n",
            "Epoch 18/300, Loss: 1.6433, Training Time: 12.54 seconds\n",
            "Epoch 19/300, Loss: 1.6363, Training Time: 12.45 seconds\n",
            "Epoch 20/300, Loss: 1.6298, Training Time: 12.48 seconds\n",
            "Epoch 21/300, Loss: 1.6236, Training Time: 12.47 seconds\n",
            "Epoch 22/300, Loss: 1.6175, Training Time: 12.58 seconds\n",
            "Epoch 23/300, Loss: 1.6113, Training Time: 12.47 seconds\n",
            "Epoch 24/300, Loss: 1.6056, Training Time: 12.60 seconds\n",
            "Epoch 25/300, Loss: 1.6003, Training Time: 12.62 seconds\n",
            "Epoch 26/300, Loss: 1.5948, Training Time: 12.61 seconds\n",
            "Epoch 27/300, Loss: 1.5894, Training Time: 12.52 seconds\n",
            "Epoch 28/300, Loss: 1.5839, Training Time: 12.38 seconds\n",
            "Epoch 29/300, Loss: 1.5787, Training Time: 12.29 seconds\n",
            "Epoch 30/300, Loss: 1.5733, Training Time: 12.36 seconds\n",
            "Epoch 31/300, Loss: 1.5685, Training Time: 12.22 seconds\n",
            "Epoch 32/300, Loss: 1.5632, Training Time: 12.26 seconds\n",
            "Epoch 33/300, Loss: 1.5588, Training Time: 12.22 seconds\n",
            "Epoch 34/300, Loss: 1.5535, Training Time: 12.34 seconds\n",
            "Epoch 35/300, Loss: 1.5482, Training Time: 12.59 seconds\n",
            "Epoch 36/300, Loss: 1.5436, Training Time: 12.56 seconds\n",
            "Epoch 37/300, Loss: 1.5388, Training Time: 12.44 seconds\n",
            "Epoch 38/300, Loss: 1.5336, Training Time: 12.45 seconds\n",
            "Epoch 39/300, Loss: 1.5290, Training Time: 12.54 seconds\n",
            "Epoch 40/300, Loss: 1.5244, Training Time: 12.46 seconds\n",
            "Epoch 41/300, Loss: 1.5195, Training Time: 12.42 seconds\n",
            "Epoch 42/300, Loss: 1.5149, Training Time: 12.48 seconds\n",
            "Epoch 43/300, Loss: 1.5094, Training Time: 12.43 seconds\n",
            "Epoch 44/300, Loss: 1.5059, Training Time: 12.48 seconds\n",
            "Epoch 45/300, Loss: 1.5005, Training Time: 12.32 seconds\n",
            "Epoch 46/300, Loss: 1.4958, Training Time: 12.38 seconds\n",
            "Epoch 47/300, Loss: 1.4917, Training Time: 12.50 seconds\n",
            "Epoch 48/300, Loss: 1.4868, Training Time: 12.50 seconds\n",
            "Epoch 49/300, Loss: 1.4820, Training Time: 12.47 seconds\n",
            "Epoch 50/300, Loss: 1.4774, Training Time: 12.26 seconds\n",
            "Epoch 51/300, Loss: 1.4725, Training Time: 12.35 seconds\n",
            "Epoch 52/300, Loss: 1.4678, Training Time: 12.09 seconds\n",
            "Epoch 53/300, Loss: 1.4631, Training Time: 12.16 seconds\n",
            "Epoch 54/300, Loss: 1.4587, Training Time: 12.14 seconds\n",
            "Epoch 55/300, Loss: 1.4540, Training Time: 12.12 seconds\n",
            "Epoch 56/300, Loss: 1.4489, Training Time: 12.16 seconds\n",
            "Epoch 57/300, Loss: 1.4446, Training Time: 12.12 seconds\n",
            "Epoch 58/300, Loss: 1.4400, Training Time: 11.95 seconds\n",
            "Epoch 59/300, Loss: 1.4353, Training Time: 12.24 seconds\n",
            "Epoch 60/300, Loss: 1.4309, Training Time: 12.28 seconds\n",
            "Epoch 61/300, Loss: 1.4258, Training Time: 12.38 seconds\n",
            "Epoch 62/300, Loss: 1.4214, Training Time: 12.17 seconds\n",
            "Epoch 63/300, Loss: 1.4174, Training Time: 12.24 seconds\n",
            "Epoch 64/300, Loss: 1.4122, Training Time: 12.24 seconds\n",
            "Epoch 65/300, Loss: 1.4080, Training Time: 12.16 seconds\n",
            "Epoch 66/300, Loss: 1.4040, Training Time: 12.20 seconds\n",
            "Epoch 67/300, Loss: 1.3987, Training Time: 12.11 seconds\n",
            "Epoch 68/300, Loss: 1.3942, Training Time: 11.86 seconds\n",
            "Epoch 69/300, Loss: 1.3899, Training Time: 12.26 seconds\n",
            "Epoch 70/300, Loss: 1.3852, Training Time: 12.44 seconds\n",
            "Epoch 71/300, Loss: 1.3802, Training Time: 12.70 seconds\n",
            "Epoch 72/300, Loss: 1.3762, Training Time: 12.70 seconds\n",
            "Epoch 73/300, Loss: 1.3718, Training Time: 12.45 seconds\n",
            "Epoch 74/300, Loss: 1.3671, Training Time: 12.45 seconds\n",
            "Epoch 75/300, Loss: 1.3627, Training Time: 12.60 seconds\n",
            "Epoch 76/300, Loss: 1.3581, Training Time: 12.41 seconds\n",
            "Epoch 77/300, Loss: 1.3538, Training Time: 12.49 seconds\n",
            "Epoch 78/300, Loss: 1.3494, Training Time: 12.65 seconds\n",
            "Epoch 79/300, Loss: 1.3441, Training Time: 12.44 seconds\n",
            "Epoch 80/300, Loss: 1.3402, Training Time: 12.43 seconds\n",
            "Epoch 81/300, Loss: 1.3356, Training Time: 12.49 seconds\n",
            "Epoch 82/300, Loss: 1.3308, Training Time: 12.36 seconds\n",
            "Epoch 83/300, Loss: 1.3264, Training Time: 12.41 seconds\n",
            "Epoch 84/300, Loss: 1.3218, Training Time: 12.37 seconds\n",
            "Epoch 85/300, Loss: 1.3175, Training Time: 12.73 seconds\n",
            "Epoch 86/300, Loss: 1.3128, Training Time: 12.70 seconds\n",
            "Epoch 87/300, Loss: 1.3086, Training Time: 12.45 seconds\n",
            "Epoch 88/300, Loss: 1.3040, Training Time: 12.44 seconds\n",
            "Epoch 89/300, Loss: 1.2996, Training Time: 12.40 seconds\n",
            "Epoch 90/300, Loss: 1.2952, Training Time: 12.43 seconds\n",
            "Epoch 91/300, Loss: 1.2903, Training Time: 12.46 seconds\n",
            "Epoch 92/300, Loss: 1.2861, Training Time: 12.37 seconds\n",
            "Epoch 93/300, Loss: 1.2822, Training Time: 12.36 seconds\n",
            "Epoch 94/300, Loss: 1.2772, Training Time: 12.34 seconds\n",
            "Epoch 95/300, Loss: 1.2724, Training Time: 12.42 seconds\n",
            "Epoch 96/300, Loss: 1.2684, Training Time: 12.18 seconds\n",
            "Epoch 97/300, Loss: 1.2636, Training Time: 12.21 seconds\n",
            "Epoch 98/300, Loss: 1.2596, Training Time: 12.46 seconds\n",
            "Epoch 99/300, Loss: 1.2549, Training Time: 12.41 seconds\n",
            "Epoch 100/300, Loss: 1.2505, Training Time: 12.58 seconds\n",
            "Epoch 101/300, Loss: 1.2461, Training Time: 12.35 seconds\n",
            "Epoch 102/300, Loss: 1.2414, Training Time: 12.38 seconds\n",
            "Epoch 103/300, Loss: 1.2377, Training Time: 12.35 seconds\n",
            "Epoch 104/300, Loss: 1.2330, Training Time: 12.30 seconds\n",
            "Epoch 105/300, Loss: 1.2280, Training Time: 12.58 seconds\n",
            "Epoch 106/300, Loss: 1.2233, Training Time: 12.36 seconds\n",
            "Epoch 107/300, Loss: 1.2192, Training Time: 12.11 seconds\n",
            "Epoch 108/300, Loss: 1.2149, Training Time: 12.08 seconds\n",
            "Epoch 109/300, Loss: 1.2101, Training Time: 11.99 seconds\n",
            "Epoch 110/300, Loss: 1.2052, Training Time: 12.40 seconds\n",
            "Epoch 111/300, Loss: 1.2014, Training Time: 12.49 seconds\n",
            "Epoch 112/300, Loss: 1.1967, Training Time: 11.92 seconds\n",
            "Epoch 113/300, Loss: 1.1928, Training Time: 11.91 seconds\n",
            "Epoch 114/300, Loss: 1.1876, Training Time: 11.87 seconds\n",
            "Epoch 115/300, Loss: 1.1838, Training Time: 12.10 seconds\n",
            "Epoch 116/300, Loss: 1.1788, Training Time: 12.12 seconds\n",
            "Epoch 117/300, Loss: 1.1744, Training Time: 11.94 seconds\n",
            "Epoch 118/300, Loss: 1.1695, Training Time: 12.20 seconds\n",
            "Epoch 119/300, Loss: 1.1660, Training Time: 12.05 seconds\n",
            "Epoch 120/300, Loss: 1.1613, Training Time: 12.06 seconds\n",
            "Epoch 121/300, Loss: 1.1568, Training Time: 11.96 seconds\n",
            "Epoch 122/300, Loss: 1.1523, Training Time: 11.92 seconds\n",
            "Epoch 123/300, Loss: 1.1476, Training Time: 12.12 seconds\n",
            "Epoch 124/300, Loss: 1.1437, Training Time: 12.02 seconds\n",
            "Epoch 125/300, Loss: 1.1391, Training Time: 12.23 seconds\n",
            "Epoch 126/300, Loss: 1.1342, Training Time: 12.28 seconds\n",
            "Epoch 127/300, Loss: 1.1293, Training Time: 12.12 seconds\n",
            "Epoch 128/300, Loss: 1.1258, Training Time: 12.30 seconds\n",
            "Epoch 129/300, Loss: 1.1213, Training Time: 12.21 seconds\n",
            "Epoch 130/300, Loss: 1.1171, Training Time: 12.24 seconds\n",
            "Epoch 131/300, Loss: 1.1121, Training Time: 12.37 seconds\n",
            "Epoch 132/300, Loss: 1.1083, Training Time: 12.25 seconds\n",
            "Epoch 133/300, Loss: 1.1035, Training Time: 12.24 seconds\n",
            "Epoch 134/300, Loss: 1.0996, Training Time: 12.25 seconds\n",
            "Epoch 135/300, Loss: 1.0950, Training Time: 12.24 seconds\n",
            "Epoch 136/300, Loss: 1.0902, Training Time: 12.36 seconds\n",
            "Epoch 137/300, Loss: 1.0859, Training Time: 12.34 seconds\n",
            "Epoch 138/300, Loss: 1.0817, Training Time: 12.29 seconds\n",
            "Epoch 139/300, Loss: 1.0770, Training Time: 12.17 seconds\n",
            "Epoch 140/300, Loss: 1.0727, Training Time: 12.17 seconds\n",
            "Epoch 141/300, Loss: 1.0686, Training Time: 12.03 seconds\n",
            "Epoch 142/300, Loss: 1.0643, Training Time: 12.00 seconds\n",
            "Epoch 143/300, Loss: 1.0597, Training Time: 11.97 seconds\n",
            "Epoch 144/300, Loss: 1.0552, Training Time: 11.99 seconds\n",
            "Epoch 145/300, Loss: 1.0509, Training Time: 12.17 seconds\n",
            "Epoch 146/300, Loss: 1.0469, Training Time: 12.04 seconds\n",
            "Epoch 147/300, Loss: 1.0420, Training Time: 12.11 seconds\n",
            "Epoch 148/300, Loss: 1.0381, Training Time: 12.06 seconds\n",
            "Epoch 149/300, Loss: 1.0345, Training Time: 12.09 seconds\n",
            "Epoch 150/300, Loss: 1.0295, Training Time: 12.25 seconds\n",
            "Epoch 151/300, Loss: 1.0251, Training Time: 12.32 seconds\n",
            "Epoch 152/300, Loss: 1.0210, Training Time: 12.03 seconds\n",
            "Epoch 153/300, Loss: 1.0164, Training Time: 12.09 seconds\n",
            "Epoch 154/300, Loss: 1.0119, Training Time: 12.01 seconds\n",
            "Epoch 155/300, Loss: 1.0074, Training Time: 12.08 seconds\n",
            "Epoch 156/300, Loss: 1.0032, Training Time: 12.04 seconds\n",
            "Epoch 157/300, Loss: 0.9988, Training Time: 11.83 seconds\n",
            "Epoch 158/300, Loss: 0.9944, Training Time: 11.83 seconds\n",
            "Epoch 159/300, Loss: 0.9908, Training Time: 11.68 seconds\n",
            "Epoch 160/300, Loss: 0.9861, Training Time: 12.01 seconds\n",
            "Epoch 161/300, Loss: 0.9824, Training Time: 11.98 seconds\n",
            "Epoch 162/300, Loss: 0.9776, Training Time: 12.14 seconds\n",
            "Epoch 163/300, Loss: 0.9734, Training Time: 12.01 seconds\n",
            "Epoch 164/300, Loss: 0.9690, Training Time: 12.11 seconds\n",
            "Epoch 165/300, Loss: 0.9649, Training Time: 12.06 seconds\n",
            "Epoch 166/300, Loss: 0.9609, Training Time: 11.92 seconds\n",
            "Epoch 167/300, Loss: 0.9563, Training Time: 11.95 seconds\n",
            "Epoch 168/300, Loss: 0.9523, Training Time: 11.91 seconds\n",
            "Epoch 169/300, Loss: 0.9482, Training Time: 12.08 seconds\n",
            "Epoch 170/300, Loss: 0.9441, Training Time: 12.05 seconds\n",
            "Epoch 171/300, Loss: 0.9396, Training Time: 11.99 seconds\n",
            "Epoch 172/300, Loss: 0.9353, Training Time: 11.96 seconds\n",
            "Epoch 173/300, Loss: 0.9313, Training Time: 11.90 seconds\n",
            "Epoch 174/300, Loss: 0.9270, Training Time: 11.97 seconds\n",
            "Epoch 175/300, Loss: 0.9235, Training Time: 11.96 seconds\n",
            "Epoch 176/300, Loss: 0.9185, Training Time: 12.00 seconds\n",
            "Epoch 177/300, Loss: 0.9141, Training Time: 11.85 seconds\n",
            "Epoch 178/300, Loss: 0.9105, Training Time: 11.98 seconds\n",
            "Epoch 179/300, Loss: 0.9064, Training Time: 12.01 seconds\n",
            "Epoch 180/300, Loss: 0.9023, Training Time: 11.92 seconds\n",
            "Epoch 181/300, Loss: 0.8979, Training Time: 11.97 seconds\n",
            "Epoch 182/300, Loss: 0.8937, Training Time: 11.88 seconds\n",
            "Epoch 183/300, Loss: 0.8903, Training Time: 11.82 seconds\n",
            "Epoch 184/300, Loss: 0.8859, Training Time: 11.89 seconds\n",
            "Epoch 185/300, Loss: 0.8820, Training Time: 12.08 seconds\n",
            "Epoch 186/300, Loss: 0.8775, Training Time: 11.79 seconds\n",
            "Epoch 187/300, Loss: 0.8736, Training Time: 11.88 seconds\n",
            "Epoch 188/300, Loss: 0.8695, Training Time: 12.02 seconds\n",
            "Epoch 189/300, Loss: 0.8656, Training Time: 11.79 seconds\n",
            "Epoch 190/300, Loss: 0.8615, Training Time: 11.73 seconds\n",
            "Epoch 191/300, Loss: 0.8576, Training Time: 11.65 seconds\n",
            "Epoch 192/300, Loss: 0.8530, Training Time: 11.79 seconds\n",
            "Epoch 193/300, Loss: 0.8494, Training Time: 11.83 seconds\n",
            "Epoch 194/300, Loss: 0.8456, Training Time: 12.00 seconds\n",
            "Epoch 195/300, Loss: 0.8415, Training Time: 11.91 seconds\n",
            "Epoch 196/300, Loss: 0.8374, Training Time: 11.97 seconds\n",
            "Epoch 197/300, Loss: 0.8338, Training Time: 11.87 seconds\n",
            "Epoch 198/300, Loss: 0.8305, Training Time: 11.96 seconds\n",
            "Epoch 199/300, Loss: 0.8259, Training Time: 11.86 seconds\n",
            "Epoch 200/300, Loss: 0.8216, Training Time: 11.91 seconds\n",
            "Epoch 201/300, Loss: 0.8176, Training Time: 11.75 seconds\n",
            "Epoch 202/300, Loss: 0.8138, Training Time: 11.80 seconds\n",
            "Epoch 203/300, Loss: 0.8100, Training Time: 11.93 seconds\n",
            "Epoch 204/300, Loss: 0.8062, Training Time: 11.98 seconds\n",
            "Epoch 205/300, Loss: 0.8023, Training Time: 11.85 seconds\n",
            "Epoch 206/300, Loss: 0.7984, Training Time: 11.86 seconds\n",
            "Epoch 207/300, Loss: 0.7943, Training Time: 11.85 seconds\n",
            "Epoch 208/300, Loss: 0.7906, Training Time: 11.84 seconds\n",
            "Epoch 209/300, Loss: 0.7872, Training Time: 12.03 seconds\n",
            "Epoch 210/300, Loss: 0.7831, Training Time: 11.98 seconds\n",
            "Epoch 211/300, Loss: 0.7789, Training Time: 11.85 seconds\n",
            "Epoch 212/300, Loss: 0.7755, Training Time: 11.95 seconds\n",
            "Epoch 213/300, Loss: 0.7713, Training Time: 11.87 seconds\n",
            "Epoch 214/300, Loss: 0.7675, Training Time: 12.09 seconds\n",
            "Epoch 215/300, Loss: 0.7640, Training Time: 11.78 seconds\n",
            "Epoch 216/300, Loss: 0.7601, Training Time: 11.55 seconds\n",
            "Epoch 217/300, Loss: 0.7569, Training Time: 11.71 seconds\n",
            "Epoch 218/300, Loss: 0.7530, Training Time: 11.68 seconds\n",
            "Epoch 219/300, Loss: 0.7490, Training Time: 11.91 seconds\n",
            "Epoch 220/300, Loss: 0.7456, Training Time: 11.94 seconds\n",
            "Epoch 221/300, Loss: 0.7410, Training Time: 11.94 seconds\n",
            "Epoch 222/300, Loss: 0.7377, Training Time: 11.77 seconds\n",
            "Epoch 223/300, Loss: 0.7337, Training Time: 11.89 seconds\n",
            "Epoch 224/300, Loss: 0.7307, Training Time: 11.80 seconds\n",
            "Epoch 225/300, Loss: 0.7268, Training Time: 12.17 seconds\n",
            "Epoch 226/300, Loss: 0.7231, Training Time: 12.11 seconds\n",
            "Epoch 227/300, Loss: 0.7194, Training Time: 11.95 seconds\n",
            "Epoch 228/300, Loss: 0.7158, Training Time: 12.08 seconds\n",
            "Epoch 229/300, Loss: 0.7124, Training Time: 12.23 seconds\n",
            "Epoch 230/300, Loss: 0.7085, Training Time: 12.09 seconds\n",
            "Epoch 231/300, Loss: 0.7050, Training Time: 12.01 seconds\n",
            "Epoch 232/300, Loss: 0.7013, Training Time: 11.89 seconds\n",
            "Epoch 233/300, Loss: 0.6984, Training Time: 12.05 seconds\n",
            "Epoch 234/300, Loss: 0.6944, Training Time: 11.85 seconds\n",
            "Epoch 235/300, Loss: 0.6909, Training Time: 12.10 seconds\n",
            "Epoch 236/300, Loss: 0.6873, Training Time: 11.97 seconds\n",
            "Epoch 237/300, Loss: 0.6840, Training Time: 11.93 seconds\n",
            "Epoch 238/300, Loss: 0.6805, Training Time: 12.02 seconds\n",
            "Epoch 239/300, Loss: 0.6766, Training Time: 11.88 seconds\n",
            "Epoch 240/300, Loss: 0.6733, Training Time: 12.30 seconds\n",
            "Epoch 241/300, Loss: 0.6702, Training Time: 12.12 seconds\n",
            "Epoch 242/300, Loss: 0.6667, Training Time: 12.03 seconds\n",
            "Epoch 243/300, Loss: 0.6630, Training Time: 12.08 seconds\n",
            "Epoch 244/300, Loss: 0.6599, Training Time: 11.85 seconds\n",
            "Epoch 245/300, Loss: 0.6566, Training Time: 12.04 seconds\n",
            "Epoch 246/300, Loss: 0.6526, Training Time: 11.88 seconds\n",
            "Epoch 247/300, Loss: 0.6497, Training Time: 11.67 seconds\n",
            "Epoch 248/300, Loss: 0.6460, Training Time: 11.88 seconds\n",
            "Epoch 249/300, Loss: 0.6431, Training Time: 11.67 seconds\n",
            "Epoch 250/300, Loss: 0.6397, Training Time: 11.87 seconds\n",
            "Epoch 251/300, Loss: 0.6360, Training Time: 11.88 seconds\n",
            "Epoch 252/300, Loss: 0.6325, Training Time: 11.92 seconds\n",
            "Epoch 253/300, Loss: 0.6293, Training Time: 11.93 seconds\n",
            "Epoch 254/300, Loss: 0.6261, Training Time: 11.87 seconds\n",
            "Epoch 255/300, Loss: 0.6233, Training Time: 11.93 seconds\n",
            "Epoch 256/300, Loss: 0.6197, Training Time: 11.95 seconds\n",
            "Epoch 257/300, Loss: 0.6165, Training Time: 11.97 seconds\n",
            "Epoch 258/300, Loss: 0.6132, Training Time: 12.01 seconds\n",
            "Epoch 259/300, Loss: 0.6097, Training Time: 11.96 seconds\n",
            "Epoch 260/300, Loss: 0.6066, Training Time: 12.22 seconds\n",
            "Epoch 261/300, Loss: 0.6033, Training Time: 12.53 seconds\n",
            "Epoch 262/300, Loss: 0.6004, Training Time: 12.26 seconds\n",
            "Epoch 263/300, Loss: 0.5970, Training Time: 12.44 seconds\n",
            "Epoch 264/300, Loss: 0.5937, Training Time: 12.32 seconds\n",
            "Epoch 265/300, Loss: 0.5905, Training Time: 12.04 seconds\n",
            "Epoch 266/300, Loss: 0.5875, Training Time: 12.34 seconds\n",
            "Epoch 267/300, Loss: 0.5844, Training Time: 11.88 seconds\n",
            "Epoch 268/300, Loss: 0.5813, Training Time: 12.10 seconds\n",
            "Epoch 269/300, Loss: 0.5781, Training Time: 11.95 seconds\n",
            "Epoch 270/300, Loss: 0.5753, Training Time: 12.05 seconds\n",
            "Epoch 271/300, Loss: 0.5723, Training Time: 12.02 seconds\n",
            "Epoch 272/300, Loss: 0.5693, Training Time: 12.04 seconds\n",
            "Epoch 273/300, Loss: 0.5658, Training Time: 12.15 seconds\n",
            "Epoch 274/300, Loss: 0.5631, Training Time: 12.05 seconds\n",
            "Epoch 275/300, Loss: 0.5600, Training Time: 12.30 seconds\n",
            "Epoch 276/300, Loss: 0.5572, Training Time: 12.16 seconds\n",
            "Epoch 277/300, Loss: 0.5541, Training Time: 12.10 seconds\n",
            "Epoch 278/300, Loss: 0.5509, Training Time: 12.14 seconds\n",
            "Epoch 279/300, Loss: 0.5480, Training Time: 12.19 seconds\n",
            "Epoch 280/300, Loss: 0.5450, Training Time: 12.28 seconds\n",
            "Epoch 281/300, Loss: 0.5425, Training Time: 12.18 seconds\n",
            "Epoch 282/300, Loss: 0.5394, Training Time: 12.21 seconds\n",
            "Epoch 283/300, Loss: 0.5362, Training Time: 12.22 seconds\n",
            "Epoch 284/300, Loss: 0.5332, Training Time: 12.28 seconds\n",
            "Epoch 285/300, Loss: 0.5310, Training Time: 12.10 seconds\n",
            "Epoch 286/300, Loss: 0.5279, Training Time: 12.07 seconds\n",
            "Epoch 287/300, Loss: 0.5247, Training Time: 12.04 seconds\n",
            "Epoch 288/300, Loss: 0.5218, Training Time: 12.22 seconds\n",
            "Epoch 289/300, Loss: 0.5191, Training Time: 12.21 seconds\n",
            "Epoch 290/300, Loss: 0.5159, Training Time: 12.00 seconds\n",
            "Epoch 291/300, Loss: 0.5133, Training Time: 12.28 seconds\n",
            "Epoch 292/300, Loss: 0.5107, Training Time: 12.20 seconds\n",
            "Epoch 293/300, Loss: 0.5079, Training Time: 12.14 seconds\n",
            "Epoch 294/300, Loss: 0.5050, Training Time: 11.75 seconds\n",
            "Epoch 295/300, Loss: 0.5026, Training Time: 11.88 seconds\n",
            "Epoch 296/300, Loss: 0.4997, Training Time: 11.79 seconds\n",
            "Epoch 297/300, Loss: 0.4968, Training Time: 11.92 seconds\n",
            "Epoch 298/300, Loss: 0.4942, Training Time: 11.91 seconds\n",
            "Epoch 299/300, Loss: 0.4918, Training Time: 12.14 seconds\n",
            "Epoch 300/300, Loss: 0.4888, Training Time: 12.16 seconds\n",
            "Total Training Time: 3651.95 seconds\n",
            "Test Accuracy: 49.54%\n",
            "Final F1 Score: 0.4942\n",
            "Final Confusion Matrix:\n",
            "[[573  38  65  34  27  25  22  34 134  48]\n",
            " [ 46 571  23  36  24  31  25  38  63 143]\n",
            " [ 79  22 372  82 147  79  90  81  28  20]\n",
            " [ 36  31  93 293  81 207 125  63  33  38]\n",
            " [ 39   6 129  66 468  70 105  73  32  12]\n",
            " [ 33  15  96 183  94 368  83  61  39  28]\n",
            " [ 12  18  90  86 101  67 569  23  17  17]\n",
            " [ 38  22  63  71  95  86  31 535  17  42]\n",
            " [ 97  38  21  30  25  34  14  17 675  49]\n",
            " [ 52 159  18  38  20  32  30  41  80 530]]\n",
            "Final Test Accuracy: 49.54%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import time\n",
        "\n",
        "# Load CIFAR-10 dataset to calculate mean and std\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Calculate mean and std from the dataset\n",
        "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
        "mean = imgs.view(3, -1).mean(dim=1)\n",
        "std = imgs.view(3, -1).std(dim=1)\n",
        "\n",
        "# Set device (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformation with calculated mean and std for normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset with normalization applied\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the neural network model\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),               # Flatten the input\n",
        "    nn.Linear(32 * 32 * 3, 512),# Fully connected layer\n",
        "    nn.Tanh(),                  # Activation function\n",
        "    nn.Linear(512, 10)          # Output layer\n",
        ").to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()  # Cross-entropy loss for classification\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)  # Stochastic Gradient Descent\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 300\n",
        "total_start_time = time.time()  # Record the total start time for training duration\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()  # Record the start time for each epoch\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimize the weights\n",
        "\n",
        "        running_loss += loss.item()  # Accumulate loss\n",
        "\n",
        "    # Calculate training time for the epoch\n",
        "    training_time = time.time() - start_time\n",
        "\n",
        "    # Print training statistics every epoch\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader):.4f}, Training Time: {training_time:.2f} seconds')\n",
        "\n",
        "# Print total training time\n",
        "total_training_time = time.time() - total_start_time\n",
        "print(f'Total Training Time: {total_training_time:.2f} seconds')\n",
        "\n",
        "# Evaluate the model on test data\n",
        "model.eval()  # Set model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "all_predicted = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Calculate and print test accuracy\n",
        "accuracy = correct / total\n",
        "print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Calculate and print F1 score\n",
        "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
        "print(f'Final F1 Score: {f1:.4f}')\n",
        "\n",
        "# Calculate and print confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predicted)\n",
        "print('Final Confusion Matrix:')\n",
        "print(cm)\n",
        "\n",
        "# Print final evaluation accuracy\n",
        "print(f'Final Test Accuracy: {accuracy * 100:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud9LdkflRfgI"
      },
      "source": [
        "Part 2 of Problem 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PGd9XjyRe81",
        "outputId": "cccb8673-8b70-460f-8d35-244d028d494b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Epoch 1/300, Loss: 2.2885811670357006, Training Time: 24.58 seconds\n",
            "Epoch 11/300, Loss: 1.8135027722324557, Training Time: 25.33 seconds\n",
            "Epoch 21/300, Loss: 1.6059201743901539, Training Time: 24.31 seconds\n",
            "Epoch 31/300, Loss: 1.4788585521680924, Training Time: 25.66 seconds\n",
            "Epoch 41/300, Loss: 1.3673661799382066, Training Time: 24.11 seconds\n",
            "Epoch 51/300, Loss: 1.2665794695277348, Training Time: 24.23 seconds\n",
            "Epoch 61/300, Loss: 1.1705999240241087, Training Time: 24.59 seconds\n",
            "Epoch 71/300, Loss: 1.072215873795702, Training Time: 24.49 seconds\n",
            "Epoch 81/300, Loss: 0.971529327085256, Training Time: 24.15 seconds\n",
            "Epoch 91/300, Loss: 0.866786785747694, Training Time: 24.45 seconds\n",
            "Epoch 101/300, Loss: 0.760510644187098, Training Time: 24.19 seconds\n",
            "Epoch 111/300, Loss: 0.6540457622703079, Training Time: 24.52 seconds\n",
            "Epoch 121/300, Loss: 0.5529929045063761, Training Time: 22.96 seconds\n",
            "Epoch 131/300, Loss: 0.46065996621575805, Training Time: 25.42 seconds\n",
            "Epoch 141/300, Loss: 0.3720705191535718, Training Time: 24.10 seconds\n",
            "Epoch 151/300, Loss: 0.2978850015632027, Training Time: 24.07 seconds\n",
            "Epoch 161/300, Loss: 0.23772411871596674, Training Time: 24.31 seconds\n",
            "Epoch 171/300, Loss: 0.18511487740804167, Training Time: 23.29 seconds\n",
            "Epoch 181/300, Loss: 0.15480822041306808, Training Time: 24.11 seconds\n",
            "Epoch 191/300, Loss: 0.09831361479275977, Training Time: 24.31 seconds\n",
            "Epoch 201/300, Loss: 0.07426662024710794, Training Time: 23.31 seconds\n",
            "Epoch 211/300, Loss: 0.05688049697348148, Training Time: 25.16 seconds\n",
            "Epoch 221/300, Loss: 0.06743073722590572, Training Time: 23.22 seconds\n",
            "Epoch 231/300, Loss: 0.0420738545608471, Training Time: 24.86 seconds\n",
            "Epoch 241/300, Loss: 0.022748915440476764, Training Time: 24.12 seconds\n",
            "Epoch 251/300, Loss: 0.02437660140116387, Training Time: 24.12 seconds\n",
            "Epoch 261/300, Loss: 0.016148680865126268, Training Time: 24.16 seconds\n",
            "Epoch 271/300, Loss: 0.013213874727530439, Training Time: 23.19 seconds\n",
            "Epoch 281/300, Loss: 0.011286627965124175, Training Time: 23.79 seconds\n",
            "Epoch 291/300, Loss: 0.009700278818840757, Training Time: 23.80 seconds\n",
            "Total Training Time: 7283.57 seconds\n",
            "Test Accuracy: 51.53%\n",
            "F1 Score: 0.5161\n",
            "Confusion Matrix:\n",
            "[[622  41  64  24  41  14  28  15 106  45]\n",
            " [ 64 583  20  22  16  12  29  19  60 175]\n",
            " [ 75  19 393  97 122 102  89  56  24  23]\n",
            " [ 31   8  95 367  78 201  96  59  18  47]\n",
            " [ 30  11 136  84 454  60  81  94  32  18]\n",
            " [ 17   8  92 228  75 409  50  65  28  28]\n",
            " [ 15  12  91 117  94  45 553  19  24  30]\n",
            " [ 33   7  58  57  93 121   9 558   7  57]\n",
            " [109  61  14  33  27  21  16  14 653  52]\n",
            " [ 57 146  12  41  13  26  30  46  68 561]]\n",
            "Final Test Accuracy: 51.53%\n",
            "Final F1 Score: 0.5161\n",
            "Final Confusion Matrix:\n",
            "[[622  41  64  24  41  14  28  15 106  45]\n",
            " [ 64 583  20  22  16  12  29  19  60 175]\n",
            " [ 75  19 393  97 122 102  89  56  24  23]\n",
            " [ 31   8  95 367  78 201  96  59  18  47]\n",
            " [ 30  11 136  84 454  60  81  94  32  18]\n",
            " [ 17   8  92 228  75 409  50  65  28  28]\n",
            " [ 15  12  91 117  94  45 553  19  24  30]\n",
            " [ 33   7  58  57  93 121   9 558   7  57]\n",
            " [109  61  14  33  27  21  16  14 653  52]\n",
            " [ 57 146  12  41  13  26  30  46  68 561]]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import confusion_matrix, f1_score\n",
        "import time\n",
        "\n",
        "# Load CIFAR-10 dataset to calculate mean and std\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "# Calculate mean and std of the training dataset\n",
        "imgs = torch.stack([img_t for img_t, _ in train_dataset], dim=3)\n",
        "mean = imgs.view(3, -1).mean(dim=1)\n",
        "std = imgs.view(3, -1).std(dim=1)\n",
        "\n",
        "# Set device to GPU if available, otherwise CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define transformation with calculated mean and std for normalization\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Load CIFAR-10 dataset with normalization for training and testing\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Define the model with two additional hidden layers\n",
        "model = nn.Sequential(\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(32 * 32 * 3, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(512, 256),  # Additional hidden layer 1\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(256, 128),  # Additional hidden layer 2\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(128, 10)\n",
        ").to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 300\n",
        "total_start_time = time.time()  # Record the total start time\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    start_time = time.time()  # Record the start time for each epoch\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()  # Zero the parameter gradients\n",
        "\n",
        "        outputs = model(inputs)  # Forward pass\n",
        "        loss = criterion(outputs, labels)  # Compute loss\n",
        "        loss.backward()  # Backward pass\n",
        "        optimizer.step()  # Optimize the weights\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Calculate and print training time for the epoch\n",
        "    end_time = time.time()\n",
        "    training_time = end_time - start_time\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Training Time: {training_time:.2f} seconds')\n",
        "\n",
        "# Calculate and print total training time\n",
        "total_end_time = time.time()\n",
        "total_training_time = total_end_time - total_start_time\n",
        "print(f'Total Training Time: {total_training_time:.2f} seconds')\n",
        "\n",
        "# Evaluate the model\n",
        "model.eval()  # Set the model to evaluation mode\n",
        "correct = 0\n",
        "total = 0\n",
        "all_predicted = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():  # No need to track gradients for evaluation\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        all_predicted.extend(predicted.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = correct / total * 100\n",
        "\n",
        "# Print evaluation accuracy\n",
        "print(f'Test Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "# Calculate and print F1 score\n",
        "f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
        "print(f'F1 Score: {f1:.4f}')\n",
        "\n",
        "# Calculate and print confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predicted)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n",
        "\n",
        "# Final evaluation after training is complete\n",
        "print(f'Final Test Accuracy: {accuracy:.2f}%')\n",
        "final_f1 = f1_score(all_labels, all_predicted, average='weighted')\n",
        "print(f'Final F1 Score: {final_f1:.4f}')\n",
        "final_cm = confusion_matrix(all_labels, all_predicted)\n",
        "print('Final Confusion Matrix:')\n",
        "print(final_cm)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}